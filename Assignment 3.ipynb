{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d1724f",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46240d43",
   "metadata": {},
   "source": [
    "#### Instructions:\n",
    "- Write modular code with relevant docstrings and comments for you to be able to use\n",
    "functions you have implemented in future assignments.\n",
    "- All theory questions and observations must be written in a markdown cell of your jupyter notebook.You can alsoadd necessary images in `imgs/` and then include it in markdown. Any other submission method for theoretical question won't be entertained.\n",
    "- Start the assignment early, push your code regularly and enjoy learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d6a65",
   "metadata": {},
   "source": [
    "### Question 1 Optimal DT from table\n",
    "**[20 points]**\\\n",
    "We will use the dataset below to learn a decision tree which predicts if people pass machine\n",
    "learning (Yes or No), based on their previous GPA (High, Medium, or Low) and whether or\n",
    "not they studied. \n",
    "\n",
    "| GPA | Studied | Passed |\n",
    "|:---:|:-------:|:------:|\n",
    "|  L  |    F    |    F   |\n",
    "|  L  |    T    |    T   |\n",
    "|  M  |    F    |    F   |\n",
    "|  M  |    T    |    T   |\n",
    "|  H  |    F    |    T   |\n",
    "|  H  |    T    |    T   |\n",
    "    \n",
    " For this problem, you can write your answers using $log_2$\n",
    ", but it may be helpful to note\n",
    "that $log_2 3 ≈ 1.6$.\n",
    "\n",
    "---\n",
    "1. What is the entropy H(Passed)?\n",
    "2. What is the entropy H(Passed | GPA)?\n",
    "3. What is the entropy H(Passed | Studied)?\n",
    "4. Draw the full decision tree that would be learned for this dataset. You do\n",
    "not need to show any calculations.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58cf408",
   "metadata": {},
   "source": [
    "### Question 2 DT loss functions\n",
    "**[10 points]**\n",
    "1. Explain Gini impurity and Entropy. \n",
    "2. What are the min and max values for both Gini impurity and Entropy\n",
    "3. Plot the Gini impurity and Entropy for $p\\in[0,1]$.\n",
    "4. Multiply Gini impurity by a factor of 2 and overlay it over entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a406ac",
   "metadata": {},
   "source": [
    "### Question 3 Training a Decision Tree  \n",
    "**[40 points]**\n",
    "\n",
    "You can download the spam dataset from the link given below. This dataset contains feature vectors and the lables of Spam/Non-Spam mails. \n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\n",
    "\n",
    "**NOTE: The last column in each row represents whether the mail is spam or non spam**\\\n",
    "Although not needed, incase you want to know what the individual columns in the feature vector means, you can read it in the documentation given below.\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de757a",
   "metadata": {},
   "source": [
    "**Download the data and load it from the code given below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "163c0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#######################\n",
    "import pandas as pd\n",
    "spam = pd.read_csv('./spambase.data', sep=',', header=None)\n",
    "\n",
    "\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffee80e",
   "metadata": {},
   "source": [
    "You can try to normalize each column (feature) separately with wither one of the following ideas. **Do not normalize labels**.\n",
    "- Shift-and-scale normalization: substract the minimum, then divide by new maximum. Now all values are between 0-1\n",
    "- Zero mean, unit variance : substract the mean, divide by the appropriate value to get variance=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e67b0584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044818</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046256</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.028886</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011454</td>\n",
       "      <td>0.029985</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>0.064836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>0.010801</td>\n",
       "      <td>0.121673</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.048458</td>\n",
       "      <td>0.142551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.056706</td>\n",
       "      <td>0.058935</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.056706</td>\n",
       "      <td>0.058935</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2    3      4         5         6         7   \\\n",
       "0  0.000000  0.044818  0.125490  0.0  0.032  0.000000  0.000000  0.000000   \n",
       "1  0.046256  0.019608  0.098039  0.0  0.014  0.047619  0.028886  0.006301   \n",
       "2  0.013216  0.000000  0.139216  0.0  0.123  0.032313  0.026135  0.010801   \n",
       "3  0.000000  0.000000  0.000000  0.0  0.063  0.000000  0.042641  0.056706   \n",
       "4  0.000000  0.000000  0.000000  0.0  0.063  0.000000  0.042641  0.056706   \n",
       "\n",
       "         8         9   ...        48        49   50        51        52  \\\n",
       "0  0.000000  0.000000  ...  0.000000  0.000000  0.0  0.023955  0.000000   \n",
       "1  0.000000  0.051705  ...  0.000000  0.013536  0.0  0.011454  0.029985   \n",
       "2  0.121673  0.013751  ...  0.002281  0.014664  0.0  0.008498  0.030651   \n",
       "3  0.058935  0.034653  ...  0.000000  0.014048  0.0  0.004218  0.000000   \n",
       "4  0.058935  0.034653  ...  0.000000  0.013843  0.0  0.004157  0.000000   \n",
       "\n",
       "         53        54        55        56  57  \n",
       "0  0.000000  0.002502  0.006007  0.017487   1  \n",
       "1  0.002421  0.003735  0.010012  0.064836   1  \n",
       "2  0.000504  0.008008  0.048458  0.142551   1  \n",
       "3  0.000000  0.002303  0.003905  0.011995   1  \n",
       "4  0.000000  0.002303  0.003905  0.011995   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#######################\n",
    "# Your code goes here #\n",
    "# iterate over the columns of spam and subtract the minimum value of each column from each value in the column\n",
    "import numpy as np\n",
    "# get number of columns in spam\n",
    "num_cols = spam.shape[1]\n",
    "print(num_cols)\n",
    "display(spam.head())\n",
    "for i in range(num_cols):\n",
    "    if i != num_cols-1:\n",
    "        spam.iloc[:,i] = spam.iloc[:,i] - np.min(spam.iloc[:,i])\n",
    "        spam.iloc[:,i] = spam.iloc[:,i] / np.max(spam.iloc[:,i])\n",
    "# convert the DataFrame spam to a numpy array except for the last column\n",
    "display(spam.head())\n",
    "spam = spam.to_numpy()\n",
    "# get the last column of spam\n",
    "spam_labels = spam[:,-1]\n",
    "# # remove the last column from spam\n",
    "# spam = np.delete(spam, -1, axis=1)\n",
    "\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef858082",
   "metadata": {},
   "source": [
    "1. Split your data into train 80% and test dataset 20% \n",
    "2. **[BONUS]** Visualize the data using PCA . You can reduce the dimension of the data if you want. Bonus marks if this increases your accuracy.\n",
    "\n",
    "*NOTE: If you are applying PCA or any other type of dimensionality reduction, do it before splitting the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "817244db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4601, 58)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3680, 57)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#######################\n",
    "# Your code goes here #\n",
    "# visualize data using pca\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(spam)\n",
    "# spam_pca = pca.transform(spam)\n",
    "# # perform pca on all columns except the last one\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(spam[:,:-1])\n",
    "# spam_pca = pca.transform(spam[:,:-1])\n",
    "# # split data into 80% training and 20% testing\n",
    "# # plot spam_pca  \n",
    "# print(spam_pca.shape)\n",
    "spam_pca=spam\n",
    "print(spam_pca.shape)\n",
    "plt.scatter(spam_pca[:,0], spam_pca[:,1], c=spam_labels)\n",
    "plt.show()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(spam_pca, spam_labels, test_size=0.2, random_state=42)\n",
    "# get last column of spam_pca\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_pca[:,:-1], spam_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6bf66",
   "metadata": {},
   "source": [
    "You need to perform a K fold validation on this and report the average training error over all the k validations. \n",
    "- For this , you need to split the training data into k splits.\n",
    "- For each split, train a decision tree model and report the training , validation and test scores.\n",
    "- Report the scores in a tabular form for each validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "604495ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   training_scores  validation_scores  testing_scores\n",
      "0         0.999396           0.899457        0.907709\n",
      "1         0.999698           0.904891        0.901194\n",
      "2         0.999396           0.883152        0.893594\n",
      "3         0.999396           0.934783        0.912052\n",
      "4         0.999698           0.918478        0.915309\n",
      "5         0.999698           0.926630        0.912052\n",
      "6         0.999698           0.929348        0.900109\n",
      "7         0.999396           0.899457        0.899023\n",
      "8         0.999396           0.915761        0.894680\n",
      "9         0.999396           0.918478        0.913138\n",
      "Mean training score:  0.9995169082125603\n",
      "Mean validation score:  0.9130434782608695\n",
      "Mean testing score:  0.9048859934853419\n"
     ]
    }
   ],
   "source": [
    "# Initialize K and split the data\n",
    "#Run the K fold Validation and report the scores\n",
    "\n",
    "#######################\n",
    "# perform a K fold validation on this and report the average training error over all the k validations\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "# initialize the list of training scores\n",
    "training_scores = []\n",
    "# initialize the list of validation scores\n",
    "validation_scores = []\n",
    "testing_scores = []\n",
    "\n",
    "for train_index, test_index in kf.split(X_train,y_train):\n",
    "    # split the data into training and validation\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[test_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[test_index]\n",
    "    \n",
    "    # train the model\n",
    "    clf.fit(X_train_fold, y_train_fold)\n",
    "    # get the training score\n",
    "    training_scores.append(clf.score(X_train_fold, y_train_fold))\n",
    "    # get the validation score\n",
    "    validation_scores.append(clf.score(X_val_fold, y_val_fold))\n",
    "    # get the testing score\n",
    "    testing_scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "# print training scores, validation scores, and testing scores in a pandas DataFrame\n",
    "print(pd.DataFrame({'training_scores': training_scores, 'validation_scores': validation_scores, 'testing_scores': testing_scores}))\n",
    "# print mean training score, mean validation score, and mean testing score\n",
    "print(\"Mean training score: \", np.mean(training_scores))\n",
    "print(\"Mean validation score: \", np.mean(validation_scores))\n",
    "print(\"Mean testing score: \", np.mean(testing_scores))\n",
    "\n",
    "\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dcdf68",
   "metadata": {},
   "source": [
    "### Question 4 Random Forest Algorithm\n",
    "**[30 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61115eaf",
   "metadata": {},
   "source": [
    "1. What is boosting, bagging and  stacking?\n",
    "Which class does random forests belong to and why? **[5 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9c366d",
   "metadata": {},
   "source": [
    "2. Implement random forest algorithm using different decision trees. **[25 points]** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "412cfb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680\n",
      "57\n",
      "Accuracy:  0.9239956568946797\n"
     ]
    }
   ],
   "source": [
    "# def random_forest_algorithm(): # Pass necessary params as per requirements\n",
    "#     #######################\n",
    "    \n",
    "#     #######################\n",
    "\n",
    "# select 7 features at random from the 57 features\n",
    "# train a decision tree on these 7 features\n",
    "# repeat this 100 times\n",
    "# get the majority vote of the 100 decision trees\n",
    "# return the majority vote\n",
    "# print number of rows in X_train\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "print(X_train.shape[0])\n",
    "# print number of columns in X_train\n",
    "print(X_train.shape[1])\n",
    "dts=[]\n",
    "dt_features = []\n",
    "\n",
    "num_trees = 101\n",
    "for i in range(num_trees):\n",
    "    # select 7 features at random from the 57 features\n",
    "    clf=DecisionTreeClassifier(random_state=0,max_depth=8, max_features=int(sqrt(57)))\n",
    "    #features = np.random.choice(57, 7, replace=False)\n",
    "    # select all samples randomly with replacement\n",
    "    samples = np.random.choice(3680, 3680, replace=True)\n",
    "    clf.fit(X_train[samples], y_train[samples])\n",
    "    # append the decision tree to the list of decision trees\n",
    "    dts.append(clf)\n",
    "    # append the features to the list of features\n",
    "    #dt_features.append(features)\n",
    "    # store the decision tree model\n",
    "    # return the majority vote\n",
    "\n",
    "# initialize the list of predictions\n",
    "predictions = []\n",
    "votes = []\n",
    "# iterate over testing samples\n",
    "for i in range(X_test.shape[0]):\n",
    "    # initialize the list of votes\n",
    "    \n",
    "    # iterate over the 100 decision trees\n",
    "    for j in range(num_trees):\n",
    "        # get the prediction of the jth decision tree\n",
    "        pred = dts[j].predict(X_test[i].reshape(1,-1))[0]\n",
    "        # append the prediction to the list of votes\n",
    "        \n",
    "        votes.append(pred)\n",
    "    # get the majority vote\n",
    "    #print(votes)\n",
    "    majority_vote = Counter(votes).most_common(1)[0][0]\n",
    "    #print(majority_vote)\n",
    "    \n",
    "    # append the majority vote to the list of predictions\n",
    "    predictions.append(majority_vote)\n",
    "    # reset the list of votes\n",
    "    votes = []\n",
    "#print(predictions)\n",
    "# print the accuracy of the predictions\n",
    "print(\"Accuracy: \", np.mean(predictions == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
